\exam{2022}
\begin{problem}{Poisson Process}
Consider a Poisson process with arrival rate $\lambda$ = 2. Assume there is one arrival in [0, 2] and there is one (possibly the same) arrival in [1, 3]. What is the probability that we have only one arrival in [0, 3]?
\end{problem}

\begin{solution}
We are looking for the following probability:
\begin{align*}
    &P(N_{0,3}=1 |N_{0,2}=1, N_{1,3}=1)
\end{align*}
This is the same as looking for these three conditions:
\begin{enumerate}
    \item $P(N_{0,1}=0)$
    \item $P(N_{1,2}=1)$
    \item $P(N_{2,3}=0)$
\end{enumerate}
For each we can use the Poisson distribution , given us the following solutions:
\begin{align*}
    &P(N_{0,1}=0)= \frac{(\lambda t) ^0 e^ {-\lambda t}}{0!}= e^{-2}\\
    &P(N_{1,2}=1)=2.e^{-2}\\
    &P(N_{2,3}=0)=e^{-2}
\end{align*}
Since each probability (in each internal) is independent from each other , we can simplify mutliply all of them with each other giving us the final solution of $2e^{-6}$.
\end{solution}

\begin{problem}{Markov Chains}
Consider a DTMC on the state space \( S = \{(i_1, j_1, i_2, j_2) | 1 \leq i_1, j_1, i_2, j_2 \leq 8\} \), that is, the state of the DTMC is characterized by marking two squares \( (i_1, j_1) \) and \( (i_2, j_2) \) on an \( 8 \times 8 \) board. Assume the transition probabilities are such that the state \( (i'_1, j'_1, i'_2, j'_2) \) visited from state \( (i_1, j_1, i_2, j_2) \) is such that \( (i'_1, j'_1) \) is a random neighbor of \( (i_1, j_1) \) and \( (i'_2, j'_2) \) is a random neighbor of \( (i_2, j_2) \). For instance, from state \( (1, 3, 4, 4) \) we move to \( (1, 4, 4, 5) \) with probability \( \frac{1}{12} = \frac{1}{3} \cdot \frac{1}{4} \) as \( (1, 3) \) has 3 neighbors and square \( (4, 4) \) has 4 neighbors. How many communicating classes does this DTMC have? What is the period of each class?
\end{problem}

\begin{solution}
  From each square it is possible to reach any other square in a finite number of steps (irreducible). Since we can return to a square in an arbitrary number of steps, it is also aperiodic. Thus, we have a \textit{single} communicating class with a period $d=1$.
\end{solution}

\begin{problem}{Markov Chains}
Consider an irreducible, positive recurrent DTMC. Let \( P \) be its transition probability matrix and \( \pi = (\pi_0, \pi_1, \ldots) \) its unique stationary distribution. Give an expression for
\[
\tilde{p}_{i,j} = \lim_{n \to \infty} P[X_n = j | X_{n+1} = i],
\]
in terms of the entries of \( P \) and \( \pi \). Define a DTMC with transition probability matrix \( \tilde{P} \) such that entry \( (i, j) \) of \( \tilde{P} \) equals \( \tilde{p}_{i,j} \). Is this DTMC positive recurrent? If so, what can you say about its stationary distribution?
\end{problem}

\begin{solution}
  Using Bayes' theorem we can write $\tilde{p}_{i,j}$ as
  \[
    \tilde{p}_{i,j} = \lim_{n \to \infty} P[X_n = j | X_{n+1} = i] = \lim_{n \to \infty} \frac{P[X_{n+1}=i | X_n = j] \cdot P[X_n=j]}{P[X_{n+1}=i]}
  \]

  Since the process is stationary, $P[X_n=j] = \pi_j$ and $P[X_{n+1}=i] = \pi_i$. Also, $P[X_{n+1}=i | X_n = j] = P_{j,i}$, where $P_{j,i}$ is the transition probability from $j$ to $i$. Thus,
  \[
    \tilde{p}_{i,j} = \frac{P_{j,i}\cdot \pi_j}{\pi_i}
  \]

  We can now define the transition matrix $\tilde{P}$ where
  \[
    \tilde{P}_{i,j}=\tilde{p}_{i,j} = \frac{P_{j,i}\cdot \pi_j}{\pi_i}
  \]

  If the original DTMC was irreducible, the reversed DTMC based on it's stationary distribution will also be irreducible, therefore positive recurrent.
  We know that $\tilde{\pi}$ must satisfy $\tilde{\pi}\tilde{P} = \tilde{\pi}$. For a fixed $j$, we find

  \[
    \tilde{\pi}_j = \sum_i\pi_i\cdot\tilde{p}_{i,j} = \sum_i\pi_i\frac{p_{j,i}\cdot \pi_j}{\pi_i}=\sum_i\pi_j\cdot  p_{j,i}= \pi_j\sum_ip_{j,i} = \pi_j \cdot 1 = \pi_j
  \]

  Thus,
  \[
    \boxed{\tilde{\pi} = \pi}
  \]
\end{solution}

\begin{problem}{Markov Chains}
Give an example of an irreducible, transient DTMC with period 2. Explain why your example has these properties.
\end{problem}
\begin{solution}
    A DTMC that is both transient and irreducible must have an infinte state space. A simple example is the BD Markov chain where $p_0 = 1$ and $p_i = \frac{1}{2}$.
\end{solution}
\begin{problem}{PASTA}
Consider a queueing system with Poisson arrivals, where the service time of customer \( n \) depends on the inter-arrival time between customer \( n - 1 \) and \( n \). Can we apply the PASTA property for this queueing system? Explain your answer.
\end{problem}

\begin{solution}
    The requirement on the independence in the PASTA theorem states that at time $t$, the system should have no information on the arrival time of future arrivals. This dependence doesn't give that information, so PASTA still holds. Local service time doesn't affect the equilibrium.
\end{solution}

\begin{problem}{Applications}
Consider a queueing network consisting of 2 queues (not necessarily M/M/1 queues). Type 1 jobs arrive at rate \( \lambda_1 \) and first visit queue 1 followed by queue 2. Type 2 jobs arrive at rate \( \lambda_2 \) and visit queue 1 only, while type 3 jobs arrive at rate \( \lambda_3 \) and only visit queue 2. Assume that the mean time that a random job spends in the queueing network equals 5, the mean time that a random type 1 or type 2 job spends in queue 1 is 3 and that the mean time that a random type 1 or 3 job spends in queue 2 is 4. Determine the arrival rates \( \lambda_1, \lambda_2 \) and \( \lambda_3 \) given that \( \lambda_1 = \lambda_2 \) and \( \lambda = \lambda_1 + \lambda_2 + \lambda_3 = 1 \). Does your answer change if type 1 jobs visit both queues in the opposite order? Explain.
\end{problem}

\begin{solution}
    Let $T$ denote the mean total time. We can write this as
    \[
        T = P_1T_1 + P_2T_2 + P_3T_3
    \]

    Where
    \begin{itemize}
        \item $T_i$ is the mean total time spent by a job of type $i$ in the network.
        \item $P_i=\frac{\lambda_i}{\lambda}$ is the proportion of jobs of type $i$.
    \end{itemize}

    Since $\lambda=1$, $P_i = \lambda_i$. We also know
    \begin{align*}
        T_1 & = T_{1,1} + T_{1,2} = 3 + 4 = 7 \\
        T_2 & = T_{2,2} = 3 \\
        T_3 & = T_{3,2} = 4
    \end{align*}

    We find
    \[
        T = 7\lambda_1 + 3\lambda_2 + 4\lambda_3 = 5
    \]

    Given that $\lambda_1 = \lambda_2$, we find that $10\lambda_1 = 5 - 4\lambda_3$.

    We also know $\lambda_1 + \lambda_2 + \lambda_3 = \lambda = 1$, thus $\lambda_3 = 1 - 2\lambda_1$.

    Using the equation above in the mean total time equation:
    \begin{align*}
        10\lambda_1 &= 5 - 4(1-2\lambda_1) = 5 - 4 + 8\lambda_1 \\
        2\lambda_1  &= 1 \\
        \lambda_1  &= \frac{1}{2}
    \end{align*}

    We conclude
    \[
        \boxed{\lambda_1 = \lambda_2 = \frac{1}{2}, \lambda_3 = 0}
    \]

    If type 1 jobs where to visit the second queue first, this wouldn't affect the mean total time in the system, therefore our answer doesn't change.
\end{solution}

\begin{problem}{Applications}
Consider an M/M/1 queue (arrival rate \( \lambda \), service rate \( \mu \)) with the additional property that when a customer arrives it starts an exponential timer with mean \( 1/\theta \) and leaves the queueing system immediately if that timer expires before its service starts. How can you model this queueing system using a CTMC? Give an expression for the stationary distribution. For which value of \( \theta \) is this distribution a Poisson distribution?
\end{problem}

\begin{solution}
    Just like the M/M/$\infty$ queue, we increase the rate of a job leaving the queue according to the number of jobs in the queue so

    \begin{align*}
        q_{i,i-1}& = \mu + \frac{i}{\theta}\\
        q_{i,i}  & = -(\lambda + \mu + \frac{i}{\theta})\\
        q_{i,i+1}& = \lambda
    \end{align*}

    with $q_{0,0} = -\lambda$. The stationary distribution $\pi$ satisfies $\pi Q=0$ and $\sum_{i=0}^\infty\pi=1$.

    Using the balance equations, for $i=0$
    \[
        \lambda\pi_0 = \left(\mu+\frac{1}{\theta}\right)\pi_1
    \]
    For $i\geq 1$
    \[
        \lambda\pi_i = \left(\mu+\frac{i+1}{\theta}\right)\pi_{i+1}
    \]

    From these terms we can express $\pi_{i+1}$ in terms of $\pi_i$. Starting from $\pi_0$, we can recursively compute
    \[
        \pi_i = \pi_0\prod_{k=1}^i\frac{\lambda}{\mu+\frac{k}{\theta}}
    \]

    We can easily see that when $\mu=0$ and $\theta=\lambda$
    \[
        \pi_i = \pi_0\prod_{k=1}^i\frac{\lambda}{\frac{k}{\lambda}}=\pi_0\prod_{k=1}^i\frac{\lambda^2}{k} = \pi_0\frac{\lambda^{2i}}{i!}
    \]

    and $\pi_0=\exp(-\lambda^2)$ as $\sum_{i=0}^\infty\pi_i=1$ must be satisfied.

    \[
        \sum_{i=0}^\infty\pi_i=\sum_{i=0}^\infty\pi_0\frac{\lambda^{2i}}{i!}=1
    \]
    $\sum_{i=0}^\infty\frac{\lambda^{2i}}{i!}$ is the Taylor expasion of $\exp(\lambda^2)$. Thus
    \[
        \pi_0\exp(\lambda^2) = 1
    \]
    Solving for $\pi_0$ gives us $\pi_0\exp(-\lambda^2)$

Like Theorem 1.4, this means the stationary distribution is Poisson with parameter $\lambda^2$.
\end{solution}
