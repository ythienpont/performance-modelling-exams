\exam{2021}
\begin{problem}{Poisson process}
Let $P_1$ and $P_2$ be two independent Poisson processes with rates
$\lambda_1$ and $\lambda_2$ . Let $N_{t,i}$ be the number of arrivals of process $P_i$ in [0, t], for i = 1, 2. Show that:
\begin{align*}
    \lambda_2^k P(N_{t,1} = N_{t,2} + k)
    &=\lambda_1^k P(N _{t,2} = N_{t,1} + k)
\end{align*}
\end{problem}

\begin{solution}
  \[
    P(N_{t,1}=N_{t,2}+k) = \sum_{n=0}^\infty P(N_{t,1}=n+k and N_{t,2}=n)
  \]
  Since $P_1$ and $P_2$ are independent we can write this probability as a product
  \[
    P(N_{t,1}=N_{t,2}+k) = \sum_{n=0}^\infty P(N_{t,1}=n+k)P(N_{t,2}=n)
  \]
  We substitute with the Poisson probabilities
  \begin{align*}
    P(N_{t,1}=N_{t,2}+k) &= \sum_{n=0}^\infty \frac{(\lambda_1t)^{(n+k)}}{(n+k)!}\exp(-\lambda_1t)\frac{(\lambda_2t)^n}{n!}\exp(-\lambda_2t) \\
    &= \exp(-(\lambda_1+\lambda_2)t)(\lambda_1t)^k\sum_{n=0}^\infty \frac{(\lambda_1t)^n}{(n+k)!}\frac{(\lambda_2t)^n}{n!} \\
    &= \exp(-(\lambda_1+\lambda_2)t)(\lambda_1t)^k\sum_{n=0}^\infty \frac{(\lambda_1\lambda_2t^2)^n}{n!(n+k)!}
  \end{align*}

  For the reversed case we find
  \[
    P(N_{t,1}=N_{t,2}+k)= \exp(-(\lambda_1+\lambda_2)t)(\lambda_2t)^k\sum_{n=0}^\infty \frac{(\lambda_1\lambda_2t^2)^n}{n!(n+k)!}
  \]
  By multiplying the probabilities with $\lambda_2^k$ and $\lambda_1^k$ respectively, we end up with the given equation.
\end{solution}

\begin{problem}{Markov Chains}
Give an example of a DTMC with 6 states, where some states have period 2 and others have period 3, while not all states are recurrent. Explain your answer. What is the maximum number of non-zero entries in \( P \) for such a chain? Explain.
\end{problem}
\begin{solution}
    \begin{figure}[h!]
        \begin{center}
            \includegraphics[height=0.5\textwidth]{img/21.2.png}
        \end{center}
        \caption{DTMC with 6 states. 1,2 have period 2, 3-5 period 3 and 6 not recurrent}
    \end{figure}
States 1-2 communicate with period of 2. States 3-5 communicate with period of 3. State 6 does not go anywhere to ensure that it is not recurrent. Writing out the matrix P gives us six non-zero entries.\\

  \textbf{NOTE}: Amount of non-zero entries is incorrect. Example is correct, but states 1 and 2 are also transient.

  A better example:
  \[
\begin{bmatrix}
  0 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 \\
  0.2 & 0 & 0.2 & 0.2 & 0.2 & 0.2 \\
  0.2 & 0.2 & 0 & 0.2 & 0.2 & 0.2 \\
  0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 & 0 & 0 \\
\end{bmatrix}
\]- Yonah

\end{solution}
\begin{problem}{Markov Chains}
Let \( Y_i \), for \( i \geq -1 \), be an infinite set of independent random variables with \( P [Y_i = -1] = P [Y_i = 0] = P [Y_i = 1] = \frac{1}{3} \). Define \( X_n = Y_{n-1} + Y_n \) for \( n \geq 0 \). Is \( (X_n)_{n \geq 0} \) a DTMC? If so, give its transition matrix \( P \); if not, explain why.
\end{problem}

\begin{solution}
  %This is not a DTMC. For example, $X_n=1$ could be given by $Y_{n-1}=1$ and $Y_{n}=0$ or $Y_{n-1}=0$ and $Y_{n}=1$. $X_{n+1} = X_{n}-Y_{n-1}$

  We have $X_0=Y_{-1}+Y_{0}$ and $X_1=Y_1+X_0-Y_{-1}$. The state $X_{n+1}$ should be fully determined by the state of our system at time $n$, but as we see we also need to know what $Y_{n-2}$ was. Therefore the Markov property does not hold and this is not a DTMC.
\end{solution}

\begin{problem}{Markov Chains}
Consider two independent Poisson processes \( P_1 \) and \( P_2 \) with rates \( \lambda_1 \) and \( \lambda_2 \). Assume we have an infinite bag, and whenever an arrival occurs of process \( P_1 \) we add a ball to the bag. If an arrival occurs of process \( P_2 \), we remove half of the balls from the bag, that is, we remove \( \lceil k/2 \rceil \) balls if the bag contains \( k \) balls (\( \lceil x \rceil \) rounds \( x \) to the smallest integer larger than or equal to \( x \)). Let \( X_t \) be the number of balls in the bag at time \( t \). Argue that \( (X_t)_{t \geq 0} \) is an irreducible CTMC. Given \( \lambda_1 \), how large must \( \lambda_2 \) be such that this CTMC is positive recurrent? Explain your answer.
\end{problem}
\begin{solution}
    Since the two Poisson processes are independent from each other, it is intuitive that the arrival rate of $P_1$ or $P_2$ is also a Poisson process.  We also have the following:
    \begin{align*}
        &k \longrightarrow k+1 \text{with rate}\lambda_1\\
        &k \longrightarrow \lceil k/2 \rceil \text{with rate} \lambda_2 \\
    \end{align*}
    Irreducible:\\
    Since it is possible to increase k to k+1, k+2,... from any k by consecutive arrivals of $P_1$. And it is possible to decrease k all the way to 0 by consecutive arrivals of $P_2$. We find that all states can be reached by all states by a sequence of events, proving the irreducibility of the CTMC.
    \\
    Positive recurrence:\\
    The expected return time to any state must be finite. This is equivalent to ensuring that $X_t$ does not "drift to infinity". The key is to balance $\lambda_1$ and $\lambda_2$.
    We have:
    \begin{itemize}
        \item Upward drift: Balls are added at a rate $\lambda_1$, causing the chain to move upward.
        \item
        Downward drift: The balls are removed at rate $\lambda_2$, and the amount removed increases with k (since
        $\lceil k/2 \rceil$ is larger for larger
        k).
    \end{itemize}
    The expected rate of change in the number of balls ($E[\Delta X]$) depends on the current state
    k:
    \begin{align*}
        &E[\Delta X]=\lambda_1-\lambda_2 E[\lceil k/2 \rceil]\\
    \end{align*}
    For large k, $[\lceil k/2 \rceil]$ approximates $E[\lceil k/2 \rceil]$.
    To ensure the process is positive recurrent, the downward drift must counteract the upward drift, requiring $\lambda_2$to be sufficiently large such that:
    \begin{align*}
        \lambda_2 > \frac{2\lambda_1}{k} \text{ for large k}
    \end{align*}
    Since k grows, we consider the expected average drift over all states. This requires:
    \begin{align*}
        \lambda_2 > 2\lambda_1
    \end{align*}
\end{solution}

\begin{problem}{Applications}
Consider 2 queueing systems. The first is an M/M/1 queue with arrival rate \( \lambda = 0.3 \) and mean service time equal to 2. The second is a Jackson network with \( M = 1 \), \( \mu_1 = 4 \), and \( p_{1,1} = \frac{1}{3} \). How should we set \( \lambda_0 \) such that the queue length distribution is the same in both queueing systems? Explain.
\end{problem}
\begin{solution}
The mean queue length of an M/M/1 queue \( L \) is computed as
\[
L = \frac{\rho}{1-\rho}
\]

For a Jackson network with a single M/M/1 queue, this is equivalent. We now need to find \( \lambda_1 \) in function of \( \lambda_0 \) to solve the problem.

In a Jackson network, \( \lambda_i \) is calculated as
\[
\lambda_i = \lambda_0 p_{0i} + \sum_{j=1}^M p_{ji} \lambda_j
\]

For our problem, we have \( p_{01} = 1 \), \( p_{11} = \frac{1}{3} \), and \( M = 1 \). The above equation simplifies to
\[
\lambda_1 = \lambda_0 + \frac{\lambda_1}{3}
\]

Rearranging, we find
\[
\lambda_1 = \frac{3\lambda_0}{2}.
\]

We now look for a \( \lambda_0 \) such that \( \rho = \frac{\lambda}{\mu} = \frac{0.3}{2} = 0.15 \) equals the system load \( \rho_1 = \frac{\lambda_1}{\mu_1} \) of the queue in our Jackson network.

\[
\rho_1 = \frac{\lambda_1}{\mu_1} = \frac{\frac{3\lambda_0}{2}}{4}.
\]

Equating \( \rho_1 \) to \( 0.15 \), we get
\[
\frac{\frac{3\lambda_0}{2}}{4} = 0.15.
\]

Simplify to solve for \( \lambda_0 \):
\[
\frac{3\lambda_0}{8} = 0.15,
\]

\[
\lambda_0 = \frac{8 \cdot 0.15}{3} = \frac{1.2}{3} = 0.4.
\]

Thus, \( \lambda_0 = 0.4 \).\end{solution}

\begin{problem}{Applications}
Consider an Erlang-C system with \( C \) servers, i.e. an M/M/C/C+Q queue with \( Q = \infty \). Assume we know that on average \( k \) jobs are waiting in the waiting room to enter a server. Does the mean time that a customer spends in the system depend on \( C \), \( k \), or both \( C \) and \( k \)? Explain.
\end{problem}
\begin{solution}
    Let $L_q=k$ be the mean waiting room length. Little's law tells us that
    \[
        L_q=\lambda W_q
    \]
    where $W_q$ is the mean waiting time in the queue. The mean total time $W$ is the sum of the mean waiting time $W_q$ and the mean service time $S=\frac{1}{\mu}$.

    \[
        W = W_q + S
    \]

    From $L_q$ we calculate $W_q=\frac{L_q}{\lambda}$. We notice that none of the formulas depend on $C$, so the mean total time $W$ depends only on $k$.
\end{solution}
